{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "# NOTE: time.sleep() func is used to make sure that as\n",
    "# browser visits pages, they are given time to load\n",
    "# before moving to the next page or pulling the soup.\n",
    "# Feel free to remove sleep timers to make it run faster.\n",
    "# but beware of errors it may throw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize browser\n",
    "executable_path = 'drivers/chromedriver'\n",
    "browser = Browser('chrome', executable_path=executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls to scrape\n",
    "news_url = 'https://mars.nasa.gov/news/'\n",
    "image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "tweet_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "facts_url = 'https://space-facts.com/mars/'\n",
    "hemis_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to news page\n",
    "browser.visit(news_url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and make soup\n",
    "news_soup = bs(browser.html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries to collect article title, snippet, and date\n",
    "news_title_query = news_soup.find(class_='content_title', string=True)\n",
    "news_snippet_query = news_soup.find(class_='article_teaser_body', string=True)\n",
    "news_date_query = news_soup.find(class_='list_date', string=True)\n",
    "\n",
    "# make a dictionary contatining the string results from the queries\n",
    "news_dict = {'news_title':news_title_query.text,\n",
    "             'news_p':news_snippet_query.text,\n",
    "             'date':news_date_query.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that date is a string, which is fine as it's just going to be posted again.\n",
    "news_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to page\n",
    "browser.visit(image_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# base url for getting HiRes images\n",
    "jpl_url = 'www.jpl.nasa.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this button click stays on the same page but opens a collapsed overlay\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this button links to another page containing the full-size image\n",
    "browser.click_link_by_partial_text('more info')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make soup\n",
    "image_soup = bs(browser.html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect image url from page, which is partial, and add the base url with string concatination\n",
    "image_query = image_soup.find(class_='main_image')\n",
    "featured_image_url = jpl_url + image_query['src']\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to twitter\n",
    "browser.visit(tweet_url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make soup\n",
    "tweet_soup = bs(browser.html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top tweet\n",
    "tweet_query = tweet_soup.find(class_='tweet-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop off image or whatever is below the weather report.\n",
    "mars_weather = tweet_query.text.split('\\n')[0]\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us built-in pandas function to scrape html table\n",
    "facts_raw_list = pd.read_html(facts_url)\n",
    "\n",
    "# result is a list of dataframes, renaming columns\n",
    "facts_raw_list[0].columns = ['feature', 'value']\n",
    "# and assign renamed dataframe to its own variable\n",
    "facts_df = facts_raw_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"table\">  <tbody>    <tr>      <td>Equatorial Diameter:</td>      <td>6,792 km</td>    </tr>    <tr>      <td>Polar Diameter:</td>      <td>6,752 km</td>    </tr>    <tr>      <td>Mass:</td>      <td>6.42 x 10^23 kg (10.7% Earth)</td>    </tr>    <tr>      <td>Moons:</td>      <td>2 (Phobos &amp; Deimos)</td>    </tr>    <tr>      <td>Orbit Distance:</td>      <td>227,943,824 km (1.52 AU)</td>    </tr>    <tr>      <td>Orbit Period:</td>      <td>687 days (1.9 years)</td>    </tr>    <tr>      <td>Surface Temperature:</td>      <td>-153 to 20 Â°C</td>    </tr>    <tr>      <td>First Record:</td>      <td>2nd millennium BC</td>    </tr>    <tr>      <td>Recorded By:</td>      <td>Egyptian astronomers</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# casting the dataframe back to html table, no index column or column headers\n",
    "facts_html = facts_df.to_html(index=False, header=False)\n",
    "\n",
    "# to_html() func places newline characters, so removing those.\n",
    "facts_html = facts_html.replace('\\n', '')\n",
    "facts_html = facts_html.replace('dataframe', 'table table-dark')\n",
    "facts_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be making a list of dictionaries, one for each hemisphere\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# go to page first to find links to click\n",
    "browser.visit(hemis_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# make soup\n",
    "hemis_soup = bs(browser.html, 'lxml')\n",
    "\n",
    "# parse for link titles\n",
    "hemis_query = hemis_soup.find_all(class_='item')\n",
    "\n",
    "# now loop through the website four times to collect images\n",
    "for result in hemis_query:\n",
    "    \n",
    "    # start with home page\n",
    "    browser.visit(hemis_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # click through to find image,\n",
    "    # using initial query results as a guide\n",
    "    browser.click_link_by_partial_text(result.h3.text)\n",
    "    \n",
    "    # make soup of page with image\n",
    "    hemis_temp_soup = bs(browser.html, 'lxml')\n",
    "    \n",
    "    # make outputs for resulting dictionary\n",
    "    hemi_name = result.h3.text.replace(' Enhanced', '')\n",
    "    hemi_img_url = hemis_temp_soup.find(target='_blank')['href']\n",
    "    \n",
    "    # and append to results list\n",
    "    hemisphere_image_urls.append({'title':hemi_name, 'img_url':hemi_img_url})\n",
    "\n",
    "    \n",
    "display(hemisphere_image_urls)\n",
    "\n",
    "# remember to close the browser!\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(hemis_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(browser.html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find_all(class_='description')\n",
    "name = result[0].h3.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.click_link_by_partial_text(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
